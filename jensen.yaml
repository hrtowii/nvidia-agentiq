# https://docs.nvidia.com/aiqtoolkit/latest/tutorials/customize-a-workflow.html
# https://docs.nvidia.com/agentiq/latest/concepts/plugins.html#entry-point
# ai agent to understand and create ai agents with agentiq
# RAG on system files
# system file search
# search online
# RAG which already has info on codebase

# mixture of agents?
# architecture - RAG, file system agent, coding + terminal agent, search agent
general:
  use_uvloop: true

memory:
  saas_memory:
    # https://docs.nvidia.com/agentiq/latest/concepts/memory.html
    _type: mem0_memory # needs MEM0_API_KEY

functions:
  # Add a tool to search wikipedia
  wikipedia_search:
    _type: wiki_search
    max_results: 5
  tavily_search:
    _type: tavily_internet_search
  # needs an api key - export TAVILY_API_KEY=<YOUR_TAVILY_API_KEY>
  current_datetime:
    _type: current_datetime
  # Exa search at 9901
  exa_search:
    _type: mcp_tool_wrapper
    url: "http://localhost:9901/sse"
    mcp_tool_name: web_search_exa
    description: "Natural language web search"

  # MCP Filesystem tools at port 9902
  read_file:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: read_file
    description: "Read the complete contents of a file from the file system."

  read_multiple_files:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: read_multiple_files
    description: "Read the contents of multiple files simultaneously."

  write_file:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: write_file
    description: "Create a new file or completely overwrite an existing file with new content."

  edit_file:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: edit_file
    description: "Make line-based edits to a text file."

  create_directory:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: create_directory
    description: "Create a new directory or ensure a directory exists."

  list_directory:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: list_directory
    description: "Get a detailed listing of all files and directories in a specified path."

  directory_tree:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: directory_tree
    description: "Get a recursive tree view of files and directories as a JSON structure."

  move_file:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: move_file
    description: "Move or rename files and directories."

  search_files:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: search_files
    description: "Recursively search for files and directories matching a pattern."

  get_file_info:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: get_file_info
    description: "Retrieve detailed metadata about a file or directory."

  list_allowed_directories:
    _type: mcp_tool_wrapper
    url: "http://localhost:9902/sse"
    mcp_tool_name: list_allowed_directories
    description: "Returns the list of directories that this server is allowed to access."

  # code execution sandbox
  code_generation:
    _type: code_generation
    programming_language: "Python"
    description: "Useful to generate Python code. For any questions about code generation, you must only use this tool!"
    llm_name: big_model
    verbose: true

  code_execution_tool:
    _type: code_execution
    programming_language: "Python"
    version: "3.11"
    description: "Useful to generate Python code. For any questions about code execution, you must only use this tool!"
    # uri: "https://emkc.org/api/v2/piston/execute" # {"message": "version is required as a string"}
    # use sandbox instead... refer to code_exec dir
    timeout: 30
    max_output_characters: 3000

  # terminal access
  terminal_tool:
    _type: mcp_tool_wrapper
    url: "http://localhost:9904/sse"
    mcp_tool_name: shell_execute
    description: "Execute a shell command"

  # nvidia_agent_iq_query:
  #   _type: webpage_query
  #   webpage_url: https://docs.nvidia.com/agentiq/latest/guides/create-customize-workflows.html
  #   description: "Search for information about LangGraph. For any questions about LangGraph, you must use this tool!"
  #   embedder_name: nv-embedqa-e5-v5
  #   chunk_size: 512

  # langsmith_query:
  #   _type: webpage_query
  #   webpage_url: https://docs.smith.langchain.com
  #   description: "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!"
  #   embedder_name: nv-embedqa-e5-v5
  #   chunk_size: 512

  # langgraph_query:
  #   _type: webpage_query
  #   webpage_url: https://langchain-ai.github.io/langgraph/tutorials/introduction
  #   description: "Search for information about LangGraph. For any questions about LangGraph, you must use this tool!"
  #   embedder_name: nv-embedqa-e5-v5
  #   chunk_size: 512

  filesystem_agent:
    _type: tool_calling_agent
    tool_names:
      - read_file
      - read_multiple_files
      - write_file
      - edit_file
      - create_directory
      - list_directory
      - directory_tree
      - move_file
      - search_files
      - get_file_info
      - list_allowed_directories
    llm_name: general_model
    verbose: true
    handle_tool_errors: true
    description: "Useful for using file system operations. If you need to create, edit, read, move and do other file operations, you must use this tool!"

  cuda_retriever_tool:
    _type: aiq_retriever
    retriever: cuda_retriever
    topic: Retrieve documentation for NVIDIA's CUDA library

  mcp_retriever_tool:
    _type: aiq_retriever
    retriever: mcp_retriever
    topic: Retrieve information about Model Context Protocol (MCP)

  nvidia_agent_iq_retriever_tool:
    _type: aiq_retriever
    retriever: mcp_retriever
    topic: Retrieve information about NVIDIA's AI Agent IQ Framework library

  add_memory:
    _type: add_memory
    memory: saas_memory
    description: |
      Add any facts about user preferences to long term memory. Always use this if users mention a preference.
      The input to this tool should be a string that describes the user's preference, not the question or answer.

  get_memory:
    _type: get_memory
    memory: saas_memory
    description: |
      Always call this tool before calling any other tools, even if the user does not mention to use it.
      The question should be about user preferences which will help you format your response.
      For example: "How does the user like responses formatted?"

  # agents
  internet_agent:
    _type: tool_calling_agent
    tool_names:
      - wikipedia_search
      - current_datetime
      - tavily_search
      - exa_search
    llm_name: general_model
    verbose: true
    handle_tool_errors: true
    description: "Useful for performing internet searches. Exa search has more tailored results for natural language queries, but be sure to use what you determine would be the best!"

  retrieval_agent:
    _type: tool_calling_agent
    tool_names:
      - nvidia_agent_iq_retriever_tool
      - mcp_retriever_tool
      - cuda_retriever_tool
    llm_name: general_model
    verbose: true
    handle_tool_errors: true
    description: "Leverages embedded documentation indices (NVIDIA AgentIQ, CUDA) to answer questions about your codebase and tools."

  code_agent:
    _type: tool_calling_agent
    tool_names:
      - terminal_tool
      - code_generation
      - code_execution_tool
    llm_name: big_model
    verbose: true
    handle_tool_errors: true
    description: "Ideal for writing, executing, and iterating on codeâ€”combines shell access with code generation and execution."

embedders:
  nv-embedqa-e5-v5:
    _type: nim
    model_name: nvidia/nv-embedqa-e5-v5
  milvus_embedder:
    _type: nim
    model_name: nvidia/nv-embedqa-e5-v5
    truncate: "END"

retrievers:
  cuda_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "cuda_docs"
    embedding_model: milvus_embedder
    top_k: 10
  mcp_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "mcp_docs"
    embedding_model: milvus_embedder
    top_k: 10
  nvidia_agent_iq_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "nvidia_aiq_docs"
    embedding_model: milvus_embedder
    top_k: 10

llms:
  #  l AIQ toolkit which LLM to use for the agent
  general_model:
    # _type: nim
    # model_name: meta/llama-3.1-70b-instruct
    _type: openai
    model_name: gpt-4o
    temperature: 0.4
    max_tokens: 1024
  smol_agent:
    # _type: nim
    # model_name: meta/llama-3.3-8b-instruct
    _type: openai
    model_name: gpt-4o
    temperature: 0.4
    max_tokens: 2000
  big_model:
    # _type: nim
    # model_name: meta/llama-3.1-405b-instruct
    _type: openai
    model_name: gpt-4o
    temperature: 0.4
    max_tokens: 1000
  openai_llm:
    _type: openai
    model_name: gpt-4o
    max_tokens: 2000

workflow:
  # Use an agent that 'reasons' and 'acts' - https://react-lm.github.io/
  # https://docs.nvidia.com/aiqtoolkit/latest/workflows/about/reasoning-agent.html
  # https://docs.nvidia.com/aiqtoolkit/latest/workflows/about/rewoo-agent.html
  _type: react_agent
  # _type: reasoning_agent
  llm_name: big_model
  # augmented_fn: react_agent
  tool_names:
    [
      get_memory,
      add_memory,
      retrieval_agent,
      code_agent,
      internet_agent,
      filesystem_agent,
    ]
  # tool_names:
  #   [
  #     exa_search,
  #     current_datetime,
  #     read_file,
  #     write_file,
  #     edit_file,
  #     create_directory,
  #     list_directory,
  #     directory_tree,
  #     move_file,
  #     search_files,
  #     get_file_info,
  #     list_allowed_directories,
  #     code_execution_tool,
  #     terminal_tool,
  #     langgraph_query,
  #     langsmith_query,
  #     nvidia_agent_iq_query,
  #   ]
  verbose: true
  retry_parsing_errors: true
  max_retries: 3
  system_prompt: |
    Answer the following questions as best you can. You may communicate and collaborate with various experts to answer the questions:

    {tools}

    Additionally, when creating or modifying the `workflow.yaml` file, you must ensure it contains:
      - A **general** section with:
        ```yaml
        general:
          use_uvloop: true
        ```
      - A **functions** section containing all tools (specify each tool's `_type`, `description`, parameters, URLs, and other required fields per AIQ Toolkit documentation).
      - An **llms** section configured as:
        ```yaml
        llms:
          nim_llm:
            _type: nim
            model_name: meta/llama-3.1-70b-instruct
            temperature: 0.5
        ```
      - A **workflow** section with:
        ```yaml
        workflow:
          _type: react_agent
          tool_names: [list_all_tool_names_here]
          llm_name: nim_llm
          verbose: true
          retry_parsing_errors: true
          max_retries: 3
        ```

    You may respond in one of two formats.
    Use the following format exactly to communicate with an expert:

    Question: the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action (if there is no required input, include "Action Input: None")
    Observation: wait for the expert to respond, do not assume the expert's response

    ... (this Thought/Action/Action Input/Observation can repeat N times.)
    Use the following format once you have the final answer:

    Thought: I now know the final answer
    Final Answer: the final answer to the original input question
